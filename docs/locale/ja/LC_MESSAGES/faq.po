# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008–2018, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
msgid ""
msgstr ""
"Project-Id-Version: Scrapy \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-09-10 09:37+0900\n"
"PO-Revision-Date: 2019-09-27 20:40+0900\n"
"Last-Translator: kuma35\n"
"Language-Team: Japanese\n"
"Language: ja_JP\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../faq.rst:4
msgid "Frequently Asked Questions"
msgstr "F.A.Q.(よくある質問と回答)"

#: ../../faq.rst:9
msgid "How does Scrapy compare to BeautifulSoup or lxml?"
msgstr "ScrapyはBeautifulSoupやlxmlと比較してどうですか？"

#: ../../faq.rst:11
msgid ""
"`BeautifulSoup`_ and `lxml`_ are libraries for parsing HTML and XML. "
"Scrapy is an application framework for writing web spiders that crawl web"
" sites and extract data from them."
msgstr "`BeautifulSoup`_ と `lxml`_ は、HTMLとXMLを解析するためのライブラリです。 Scrapyは、Webサイトをクロールし、そこからデータを抽出するWebスパイダーを作成するためのアプリケーションフレームワークです。"

#: ../../faq.rst:15
msgid ""
"Scrapy provides a built-in mechanism for extracting data (called "
":ref:`selectors <topics-selectors>`) but you can easily use "
"`BeautifulSoup`_ (or `lxml`_) instead, if you feel more comfortable "
"working with them. After all, they're just parsing libraries which can be"
" imported and used from any Python code."
msgstr "Scrapyはデータを抽出するための組み込みメカニズムを提供します( :ref:`セレクター<topics-selectors>` とよばれます)が、より快適に作業できる場合は、代わりに `BeautifulSoup`_ (または `lxml`_ )を簡単に使用できます。結局のところ、それらは任意のPythonコードからインポートして使用できるライブラリを利用しているだけです。"

#: ../../faq.rst:21
msgid ""
"In other words, comparing `BeautifulSoup`_ (or `lxml`_) to Scrapy is like"
" comparing `jinja2`_ to `Django`_."
msgstr "いいかえると、 `BeautifulSoup`_ (または `lxml`_ )とScrapyを比較することは、  `jinja2`_ と `Django`_ を比較するようなものです。"

#: ../../faq.rst:30
msgid "Can I use Scrapy with BeautifulSoup?"
msgstr "ScrapyでBeautifulSoupを使用できますか？"

#: ../../faq.rst:32
msgid ""
"Yes, you can. As mentioned :ref:`above <faq-scrapy-bs-cmp>`, "
"`BeautifulSoup`_ can be used for parsing HTML responses in Scrapy "
"callbacks. You just have to feed the response's body into a "
"``BeautifulSoup`` object and extract whatever data you need from it."
msgstr ""

#: ../../faq.rst:38
msgid ""
"Here's an example spider using BeautifulSoup API, with ``lxml`` as the "
"HTML parser::"
msgstr ""

#: ../../faq.rst:62
msgid ""
"``BeautifulSoup`` supports several HTML/XML parsers. See `BeautifulSoup's"
" official documentation`_ on which ones are available."
msgstr ""

#: ../../faq.rst:70
msgid "What Python versions does Scrapy support?"
msgstr "ScrapyはどのPythonバージョンをサポートしていますか？"

#: ../../faq.rst:72
msgid ""
"Scrapy is supported under Python 2.7 and Python 3.5+ under CPython "
"(default Python implementation) and PyPy (starting with PyPy 5.9). Python"
" 2.6 support was dropped starting at Scrapy 0.20. Python 3 support was "
"added in Scrapy 1.1. PyPy support was added in Scrapy 1.4, PyPy3 support "
"was added in Scrapy 1.5."
msgstr ""

#: ../../faq.rst:79
msgid ""
"For Python 3 support on Windows, it is recommended to use "
"Anaconda/Miniconda as :ref:`outlined in the installation guide <intro-"
"install-windows>`."
msgstr ""

#: ../../faq.rst:83
msgid "Did Scrapy \"steal\" X from Django?"
msgstr "ScrapyはDjangoからhogehogeを盗んだ？"

#: ../../faq.rst:85
msgid ""
"Probably, but we don't like that word. We think Django_ is a great open "
"source project and an example to follow, so we've used it as an "
"inspiration for Scrapy."
msgstr ""

#: ../../faq.rst:89
msgid ""
"We believe that, if something is already done well, there's no need to "
"reinvent it. This concept, besides being one of the foundations for open "
"source and free software, not only applies to software but also to "
"documentation, procedures, policies, etc. So, instead of going through "
"each problem ourselves, we choose to copy ideas from those projects that "
"have already solved them properly, and focus on the real problems we need"
" to solve."
msgstr ""

#: ../../faq.rst:96
msgid ""
"We'd be proud if Scrapy serves as an inspiration for other projects. Feel"
" free to steal from us!"
msgstr ""

#: ../../faq.rst:100
msgid "Does Scrapy work with HTTP proxies?"
msgstr "ScrapyはHTTPプロキシ経由で動作しますか？"

#: ../../faq.rst:102
msgid ""
"Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the "
"HTTP Proxy downloader middleware. See "
":class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware`."
msgstr ""

#: ../../faq.rst:107
msgid "How can I scrape an item with attributes in different pages?"
msgstr "異なるページの属性を持つアイテムをスクレイピングするにはどうすればよいですか？"

#: ../../faq.rst:109
msgid "See :ref:`topics-request-response-ref-request-callback-arguments`."
msgstr ""

#: ../../faq.rst:113
msgid "Scrapy crashes with: ImportError: No module named win32api"
msgstr "Scrapyがクラッシュします。「ImportError: No module named win32api」"

#: ../../faq.rst:115
msgid "You need to install `pywin32`_ because of `this Twisted bug`_."
msgstr ""

#: ../../faq.rst:121
msgid "How can I simulate a user login in my spider?"
msgstr "スパイダーでユーザーログインをシミュレートするにはどうすればよいですか？"

#: ../../faq.rst:123
msgid "See :ref:`topics-request-response-ref-request-userlogin`."
msgstr ""

#: ../../faq.rst:128
msgid "Does Scrapy crawl in breadth-first or depth-first order?"
msgstr "Scrapyは幅(breadth)優先または深さ(depth)優先でクロールしますか？"

#: ../../faq.rst:130
msgid ""
"By default, Scrapy uses a `LIFO`_ queue for storing pending requests, "
"which basically means that it crawls in `DFO order`_. This order is more "
"convenient in most cases."
msgstr "デフォルトでは、Scrapyは保留中のリクエストを保存するために `LIFO`_ キューを使用します。これは基本的に、DFO順序(`DFO order`_)でクロールすることを意味します。 ほとんどの場合、この順序の方が便利です。"

#: ../../faq.rst:134
msgid ""
"If you do want to crawl in true `BFO order`_, you can do it by setting "
"the following settings::"
msgstr "あなたが本当にBFO順(`BFO order`_)でクロールしたい場合は、次の設定を行うことで実行できます::"

#: ../../faq.rst:141
msgid ""
"While pending requests are below the configured values of "
":setting:`CONCURRENT_REQUESTS`, :setting:`CONCURRENT_REQUESTS_PER_DOMAIN`"
" or :setting:`CONCURRENT_REQUESTS_PER_DOMAIN`, those requests are sent "
"concurrently. As a result, the first few requests of a crawl rarely "
"follow the desired order. Lowering those settings to ``1`` enforces the "
"desired order, but it significantly slows down the crawl as a whole."
msgstr "保留中のリクエストが :setting:`CONCURRENT_REQUESTS` または :setting:`CONCURRENT_REQUESTS_PER_DOMAIN` または :setting:`CONCURRENT_REQUESTS_PER_DOMAIN` の設定値を下回っている間、これらのリクエストは同時に送信されます。その結果、クロールの最初のいくつかのリクエストが目的の順序に従うことはほとんどありません。 これらの設定を ``1`` に下げると、目的の順序が強制されますが、クロール全体が大幅に遅くなります。"

#: ../../faq.rst:150
msgid "My Scrapy crawler has memory leaks. What can I do?"
msgstr "Scrapyクローラーにメモリリークがあります。 何か私にできる事がありますか？"

#: ../../faq.rst:152
msgid "See :ref:`topics-leaks`."
msgstr ""

#: ../../faq.rst:154
msgid ""
"Also, Python has a builtin memory leak issue which is described in :ref"
":`topics-leaks-without-leaks`."
msgstr ""

#: ../../faq.rst:158
msgid "How can I make Scrapy consume less memory?"
msgstr "Scrapyが消費するメモリを減らすにはどうすればよいですか？"

#: ../../faq.rst:160
msgid "See previous question."
msgstr "1つ前の質問を見て下さい。"

#: ../../faq.rst:163
msgid "Can I use Basic HTTP Authentication in my spiders?"
msgstr "スパイダーは基本HTTP認証を使用できますか？"

#: ../../faq.rst:165
msgid ""
"Yes, see "
":class:`~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware`."
msgstr "はい。 :class:`~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware` 参照。"

#: ../../faq.rst:168
msgid "Why does Scrapy download pages in English instead of my native language?"
msgstr "Scrapyが母国語ではなく英語でページをダウンロードするのはなぜですか？"

#: ../../faq.rst:170
msgid ""
"Try changing the default `Accept-Language`_ request header by overriding "
"the :setting:`DEFAULT_REQUEST_HEADERS` setting."
msgstr ":setting:`DEFAULT_REQUEST_HEADERS` 設定をオーバーライドして、デフォルトの `Accept-Language`_ リクエスト・ヘッダーを変更してみてください。"

#: ../../faq.rst:176
msgid "Where can I find some example Scrapy projects?"
msgstr "Scrapyプロジェクトの例はどこにありますか？"

#: ../../faq.rst:178
msgid "See :ref:`intro-examples`."
msgstr ":ref:`intro-examples` 参照。"

#: ../../faq.rst:181
msgid "Can I run a spider without creating a project?"
msgstr "プロジェクトを作成せずにスパイダーを実行できますか？"

#: ../../faq.rst:183
msgid ""
"Yes. You can use the :command:`runspider` command. For example, if you "
"have a spider written in a ``my_spider.py`` file you can run it with::"
msgstr ""

#: ../../faq.rst:188
msgid "See :command:`runspider` command for more info."
msgstr ""

#: ../../faq.rst:191
msgid "I get \"Filtered offsite request\" messages. How can I fix them?"
msgstr "\"Filtered offsite request\"(フィルターされたオフサイト要求)メッセージが表示されます。 どうすれば修正できますか？"

#: ../../faq.rst:193
msgid ""
"Those messages (logged with ``DEBUG`` level) don't necessarily mean there"
" is a problem, so you may not need to fix them."
msgstr ""

#: ../../faq.rst:196
msgid ""
"Those messages are thrown by the Offsite Spider Middleware, which is a "
"spider middleware (enabled by default) whose purpose is to filter out "
"requests to domains outside the ones covered by the spider."
msgstr ""

#: ../../faq.rst:200
msgid ""
"For more info see: "
":class:`~scrapy.spidermiddlewares.offsite.OffsiteMiddleware`."
msgstr ""

#: ../../faq.rst:204
msgid "What is the recommended way to deploy a Scrapy crawler in production?"
msgstr "Scrapyクローラーを運用環境に展開する推奨方法は何ですか？"

#: ../../faq.rst:206
msgid "See :ref:`topics-deploy`."
msgstr ":ref:`topics-deploy` 参照。"

#: ../../faq.rst:209
msgid "Can I use JSON for large exports?"
msgstr "大規模なエクスポートにJSONを使用できますか？"

#: ../../faq.rst:211
msgid ""
"It'll depend on how large your output is. See :ref:`this warning <json-"
"with-large-data>` in :class:`~scrapy.exporters.JsonItemExporter` "
"documentation."
msgstr ""

#: ../../faq.rst:216
msgid "Can I return (Twisted) deferreds from signal handlers?"
msgstr "シグナルハンドラーから(Twisted)遅延(deferred)を返すことができますか？"

#: ../../faq.rst:218
msgid ""
"Some signals support returning deferreds from their handlers, others "
"don't. See the :ref:`topics-signals-ref` to know which ones."
msgstr ""

#: ../../faq.rst:222
msgid "What does the response status code 999 means?"
msgstr "レスポンスステータスコード999の意味は何ですか？"

#: ../../faq.rst:224
msgid ""
"999 is a custom response status code used by Yahoo sites to throttle "
"requests. Try slowing down the crawling speed by using a download delay "
"of ``2`` (or higher) in your spider::"
msgstr ""

#: ../../faq.rst:236
msgid ""
"Or by setting a global download delay in your project with the "
":setting:`DOWNLOAD_DELAY` setting."
msgstr ""

#: ../../faq.rst:240
msgid "Can I call ``pdb.set_trace()`` from my spiders to debug them?"
msgstr "スパイダーから ``pdb.set_trace()`` を呼び出してデバッグできますか？"

#: ../../faq.rst:242
msgid ""
"Yes, but you can also use the Scrapy shell which allows you to quickly "
"analyze (and even modify) the response being processed by your spider, "
"which is, quite often, more useful than plain old ``pdb.set_trace()``."
msgstr ""

#: ../../faq.rst:246
msgid "For more info see :ref:`topics-shell-inspect-response`."
msgstr ""

#: ../../faq.rst:249
msgid "Simplest way to dump all my scraped items into a JSON/CSV/XML file?"
msgstr "スクレイピングしたすべてのアイテムをJSON/CSV/XMLファイルにダンプする最も簡単な方法は？"

#: ../../faq.rst:251
msgid "To dump into a JSON file::"
msgstr "JSONファイルにダンプするには::"

#: ../../faq.rst:255
msgid "To dump into a CSV file::"
msgstr "CSVファイルにダンプするには::"

#: ../../faq.rst:259
msgid "To dump into a XML file::"
msgstr "XMLファイルにダンプするには::"

#: ../../faq.rst:263
msgid "For more information see :ref:`topics-feed-exports`"
msgstr ""

#: ../../faq.rst:266
msgid "What's this huge cryptic ``__VIEWSTATE`` parameter used in some forms?"
msgstr "いくつかのフォームで使用されているこの巨大な ``__VIEWSTATE`` パラメーターは何ですか？"

#: ../../faq.rst:268
msgid ""
"The ``__VIEWSTATE`` parameter is used in sites built with ASP.NET/VB.NET."
" For more info on how it works see `this page`_. Also, here's an `example"
" spider`_ which scrapes one of these sites."
msgstr ""

#: ../../faq.rst:276
msgid "What's the best way to parse big XML/CSV data feeds?"
msgstr "大きなXML/CSVデータフィードを解析する最良の方法は何ですか？"

#: ../../faq.rst:278
msgid ""
"Parsing big feeds with XPath selectors can be problematic since they need"
" to build the DOM of the entire feed in memory, and this can be quite "
"slow and consume a lot of memory."
msgstr ""

#: ../../faq.rst:282
msgid ""
"In order to avoid parsing all the entire feed at once in memory, you can "
"use the functions ``xmliter`` and ``csviter`` from "
"``scrapy.utils.iterators`` module. In fact, this is what the feed spiders"
" (see :ref:`topics-spiders`) use under the cover."
msgstr ""

#: ../../faq.rst:288
msgid "Does Scrapy manage cookies automatically?"
msgstr "ScrapyはCookieを自動的に管理しますか？"

#: ../../faq.rst:290
msgid ""
"Yes, Scrapy receives and keeps track of cookies sent by servers, and "
"sends them back on subsequent requests, like any regular web browser "
"does."
msgstr ""

#: ../../faq.rst:293
msgid "For more info see :ref:`topics-request-response` and :ref:`cookies-mw`."
msgstr ""

#: ../../faq.rst:296
msgid "How can I see the cookies being sent and received from Scrapy?"
msgstr "Scrapyとの間で送受信されているCookieを確認するにはどうすればよいですか？"

#: ../../faq.rst:298
msgid "Enable the :setting:`COOKIES_DEBUG` setting."
msgstr ""

#: ../../faq.rst:301
msgid "How can I instruct a spider to stop itself?"
msgstr "スパイダーに自分自身を止めるように指示するにはどうすればよいですか？"

#: ../../faq.rst:303
msgid ""
"Raise the :exc:`~scrapy.exceptions.CloseSpider` exception from a "
"callback. For more info see: :exc:`~scrapy.exceptions.CloseSpider`."
msgstr ""

#: ../../faq.rst:307
msgid "How can I prevent my Scrapy bot from getting banned?"
msgstr "ScrapyボットがBANされるのを防ぐにはどうすればよいですか？"

#: ../../faq.rst:309
msgid "See :ref:`bans`."
msgstr ":ref:`bans` 参照。"

#: ../../faq.rst:312
msgid "Should I use spider arguments or settings to configure my spider?"
msgstr "スパイダーを設定するには、スパイダーの引数または設定を使用する必要がありますか？"

#: ../../faq.rst:314
msgid ""
"Both :ref:`spider arguments <spiderargs>` and :ref:`settings <topics-"
"settings>` can be used to configure your spider. There is no strict rule "
"that mandates to use one or the other, but settings are more suited for "
"parameters that, once set, don't change much, while spider arguments are "
"meant to change more often, even on each spider run and sometimes are "
"required for the spider to run at all (for example, to set the start url "
"of a spider)."
msgstr ""

#: ../../faq.rst:321
msgid ""
"To illustrate with an example, assuming you have a spider that needs to "
"log into a site to scrape data, and you only want to scrape data from a "
"certain section of the site (which varies each time). In that case, the "
"credentials to log in would be settings, while the url of the section to "
"scrape would be a spider argument."
msgstr ""

#: ../../faq.rst:328
msgid "I'm scraping a XML document and my XPath selector doesn't return any items"
msgstr "XMLドキュメントをスクレイピングしていますが、XPathセレクターはアイテムを返しません"

#: ../../faq.rst:330
msgid "You may need to remove namespaces. See :ref:`removing-namespaces`."
msgstr "名前空間を削除する必要がある場合があります。 :ref:`removing-namespaces` 参照。"

#: ../../faq.rst:335
msgid "How to split an item into multiple items in an item pipeline?"
msgstr "アイテムパイプラインでアイテムを複数のアイテムに分割する方法は？"

#: ../../faq.rst:337
msgid ""
":ref:`Item pipelines <topics-item-pipeline>` cannot yield multiple items "
"per input item. :ref:`Create a spider middleware <custom-spider-"
"middleware>` instead, and use its "
":meth:`~scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output` "
"method for this puspose. For example::"
msgstr ""

