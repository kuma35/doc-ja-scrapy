# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008–2018, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
msgid ""
msgstr ""
"Project-Id-Version: Scrapy \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-09-29 08:45+0900\n"
"PO-Revision-Date: 2019-10-07 06:28+0900\n"
"Last-Translator: kuma35\n"
"Language: ja_JP\n"
"Language-Team: Japanese\n"
"Plural-Forms: nplurals=1; plural=0\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../faq.rst:4
msgid "Frequently Asked Questions"
msgstr "F.A.Q.(よくある質問と回答)"

#: ../../faq.rst:9
msgid "How does Scrapy compare to BeautifulSoup or lxml?"
msgstr "ScrapyはBeautifulSoupやlxmlと比較してどうですか？"

#: ../../faq.rst:11
msgid ""
"`BeautifulSoup`_ and `lxml`_ are libraries for parsing HTML and XML. "
"Scrapy is an application framework for writing web spiders that crawl web"
" sites and extract data from them."
msgstr ""
"`BeautifulSoup`_ と `lxml`_ は、HTMLとXMLを解析するためのライブラリです。 "
"Scrapyは、Webサイトをクロールし、そこからデータを抽出するWebスパイダーを作成するためのアプリケーションフレームワークです。"

#: ../../faq.rst:15
msgid ""
"Scrapy provides a built-in mechanism for extracting data (called "
":ref:`selectors <topics-selectors>`) but you can easily use "
"`BeautifulSoup`_ (or `lxml`_) instead, if you feel more comfortable "
"working with them. After all, they're just parsing libraries which can be"
" imported and used from any Python code."
msgstr ""
"Scrapyはデータを抽出するための組み込みメカニズムを提供します( :ref:`セレクター<topics-selectors>` "
"とよばれます)が、より快適に作業できる場合は、代わりに `BeautifulSoup`_ (または `lxml`_ "
")を簡単に使用できます。結局のところ、それらは任意のPythonコードからインポートして使用できるライブラリを利用しているだけです。"

#: ../../faq.rst:21
msgid ""
"In other words, comparing `BeautifulSoup`_ (or `lxml`_) to Scrapy is like"
" comparing `jinja2`_ to `Django`_."
msgstr ""
"いいかえると、 `BeautifulSoup`_ (または `lxml`_ )とScrapyを比較することは、  `jinja2`_ と "
"`Django`_ を比較するようなものです。"

#: ../../faq.rst:30
msgid "Can I use Scrapy with BeautifulSoup?"
msgstr "ScrapyでBeautifulSoupを使用できますか？"

#: ../../faq.rst:32
msgid ""
"Yes, you can. As mentioned :ref:`above <faq-scrapy-bs-cmp>`, "
"`BeautifulSoup`_ can be used for parsing HTML responses in Scrapy "
"callbacks. You just have to feed the response's body into a "
"``BeautifulSoup`` object and extract whatever data you need from it."
msgstr ""
"はい、できます。 :ref:`上記<faq-scrapy-bs-cmp>` のように、 `BeautifulSoup`_ "
"はScrapyコールバックでHTMLレスポンスをパースするために使用できます。 レスポンスのボディを ``BeautifulSoup`` "
"オブジェクトに送り、必要なデータを抽出するだけです。"

#: ../../faq.rst:38
msgid ""
"Here's an example spider using BeautifulSoup API, with ``lxml`` as the "
"HTML parser::"
msgstr "HTMLパーサーとして ``lxml`` を使用して、BeautifulSoup API を使用するスパイダーの例を次に示します::"

#: ../../faq.rst:62
msgid ""
"``BeautifulSoup`` supports several HTML/XML parsers. See `BeautifulSoup's"
" official documentation`_ on which ones are available."
msgstr ""
"``BeautifulSoup`` "
"はいくつかのHTML/XMLパーサーをサポートしています。利用可能なものについてはBeautifulSoupの公式ドキュメント(`BeautifulSoup's"
" official documentation`_) を参照してください。"

#: ../../faq.rst:70
msgid "What Python versions does Scrapy support?"
msgstr "ScrapyはどのPythonバージョンをサポートしていますか？"

#: ../../faq.rst:72
msgid ""
"Scrapy is supported under Python 2.7 and Python 3.5+ under CPython "
"(default Python implementation) and PyPy (starting with PyPy 5.9). Python"
" 2.6 support was dropped starting at Scrapy 0.20. Python 3 support was "
"added in Scrapy 1.1. PyPy support was added in Scrapy 1.4, PyPy3 support "
"was added in Scrapy 1.5."
msgstr ""
"ScrapyはCPython(デフォルトのPython実装)での Python 2.7 および Python 3.5+ と、PyPy(PyPy "
"5.9以降) でサポートされています。Python 2.6のサポートはScrapy 0.20以降は削除されました。Python "
"3のサポートはScrapy 1.1で追加されました。 PyPyサポートはScrapy 1.4で追加され、PyPy3サポートはScrapy "
"1.5で追加されました。"

#: ../../faq.rst:79
msgid ""
"For Python 3 support on Windows, it is recommended to use "
"Anaconda/Miniconda as :ref:`outlined in the installation guide <intro-"
"install-windows>`."
msgstr ""
"WindowsでPython 3をサポートするには、 :ref:`インストールガイドで概説<intro-install-windows>` "
"しているように、Anaconda/Miniconda を使用することをお勧めします。"

#: ../../faq.rst:83
msgid "Did Scrapy \"steal\" X from Django?"
msgstr "ScrapyはDjangoからhogehogeを盗んだ？"

#: ../../faq.rst:85
msgid ""
"Probably, but we don't like that word. We think Django_ is a great open "
"source project and an example to follow, so we've used it as an "
"inspiration for Scrapy."
msgstr ""
"たぶん。だけど、私たちはそういう言い方はしないな。 Django_ "
"は素晴らしいオープンソースプロジェクトであり、従うべき例であると考えているため、Scrapyの着想を得るのに利用したんだよ。"

#: ../../faq.rst:89
msgid ""
"We believe that, if something is already done well, there's no need to "
"reinvent it. This concept, besides being one of the foundations for open "
"source and free software, not only applies to software but also to "
"documentation, procedures, policies, etc. So, instead of going through "
"each problem ourselves, we choose to copy ideas from those projects that "
"have already solved them properly, and focus on the real problems we need"
" to solve."
msgstr ""
"車輪の再発明する必要はないという信念です。この信念は、オープンソースおよびフリーソフトウェアの基礎の1つであることに加えて、ソフトウェアだけでなく、ドキュメント、手順、ポリシーなどにも適用されます。したがって、各問題を自分で進めるのではなく、それらのプロジェクトからアイデアをコピーすることを選択します"
" それが既に各問題を適切に解決しているので、私たちは解決する必要がある実際の問題に焦点を当てる事ができます。"

#: ../../faq.rst:96
msgid ""
"We'd be proud if Scrapy serves as an inspiration for other projects. Feel"
" free to steal from us!"
msgstr "私たちはScrapyが他のプロジェクトのインスピレーションとして役立つことを誇りに思います。 じゃんじゃん盗め！"

#: ../../faq.rst:100
msgid "Does Scrapy work with HTTP proxies?"
msgstr "ScrapyはHTTPプロキシ経由で動作しますか？"

#: ../../faq.rst:102
msgid ""
"Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the "
"HTTP Proxy downloader middleware. See "
":class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware`."
msgstr ""
"はい。 HTTPプロキシのサポートは、HTTPプロキシ・ダウンローダー・ミドルウェアを通じて提供されます(Scrapy 0.8以降)。 "
":class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` "
"を参照してください。"

#: ../../faq.rst:107
msgid "How can I scrape an item with attributes in different pages?"
msgstr "異なるページの属性を持つアイテムをスクレイピングするにはどうすればよいですか？"

#: ../../faq.rst:109
msgid "See :ref:`topics-request-response-ref-request-callback-arguments`."
msgstr ":ref:`topics-request-response-ref-request-callback-arguments` 参照。"

#: ../../faq.rst:113
msgid "Scrapy crashes with: ImportError: No module named win32api"
msgstr "Scrapyがクラッシュします。「ImportError: No module named win32api」"

#: ../../faq.rst:115
msgid "You need to install `pywin32`_ because of `this Twisted bug`_."
msgstr "あなたは「このツイストバグ」(`this Twisted bug`_)のため、 `pywin32`_ をインストールする必要があります。"

#: ../../faq.rst:121
msgid "How can I simulate a user login in my spider?"
msgstr "スパイダーでユーザーログインをシミュレートするにはどうすればよいですか？"

#: ../../faq.rst:123
msgid "See :ref:`topics-request-response-ref-request-userlogin`."
msgstr ":ref:`topics-request-response-ref-request-userlogin` 参照。"

#: ../../faq.rst:128
msgid "Does Scrapy crawl in breadth-first or depth-first order?"
msgstr "Scrapyは幅(breadth)優先または深さ(depth)優先でクロールしますか？"

#: ../../faq.rst:130
msgid ""
"By default, Scrapy uses a `LIFO`_ queue for storing pending requests, "
"which basically means that it crawls in `DFO order`_. This order is more "
"convenient in most cases."
msgstr ""
"デフォルトでは、Scrapyは保留中のリクエストを保存するために `LIFO`_ キューを使用します。これは基本的に、DFO順序(`DFO "
"order`_)でクロールすることを意味します。 ほとんどの場合、この順序の方が便利です。"

#: ../../faq.rst:134
msgid ""
"If you do want to crawl in true `BFO order`_, you can do it by setting "
"the following settings::"
msgstr "あなたが本当にBFO順(`BFO order`_)でクロールしたい場合は、次の設定を行うことで実行できます::"

#: ../../faq.rst:141
msgid ""
"While pending requests are below the configured values of "
":setting:`CONCURRENT_REQUESTS`, :setting:`CONCURRENT_REQUESTS_PER_DOMAIN`"
" or :setting:`CONCURRENT_REQUESTS_PER_DOMAIN`, those requests are sent "
"concurrently. As a result, the first few requests of a crawl rarely "
"follow the desired order. Lowering those settings to ``1`` enforces the "
"desired order, but it significantly slows down the crawl as a whole."
msgstr ""
"保留中のリクエストが :setting:`CONCURRENT_REQUESTS` または "
":setting:`CONCURRENT_REQUESTS_PER_DOMAIN` または "
":setting:`CONCURRENT_REQUESTS_PER_DOMAIN` "
"の設定値を下回っている間、これらのリクエストは同時に送信されます。その結果、クロールの最初のいくつかのリクエストが目的の順序に従うことはほとんどありません。"
" これらの設定を ``1`` に下げると、目的の順序が強制されますが、クロール全体が大幅に遅くなります。"

#: ../../faq.rst:150
msgid "My Scrapy crawler has memory leaks. What can I do?"
msgstr "Scrapyクローラーにメモリリークがあります。 何か私にできる事がありますか？"

#: ../../faq.rst:152
msgid "See :ref:`topics-leaks`."
msgstr ":ref:`topics-leaks` 参照。"

#: ../../faq.rst:154
msgid ""
"Also, Python has a builtin memory leak issue which is described in :ref"
":`topics-leaks-without-leaks`."
msgstr "また、Pythonには :ref:`topics-leaks-without-leaks` で説明されている組み込みのメモリリークの問題があります。"

#: ../../faq.rst:158
msgid "How can I make Scrapy consume less memory?"
msgstr "Scrapyが消費するメモリを減らすにはどうすればよいですか？"

#: ../../faq.rst:160
msgid "See previous question."
msgstr "1つ前の質問を見て下さい。"

#: ../../faq.rst:163
msgid "Can I use Basic HTTP Authentication in my spiders?"
msgstr "スパイダーは基本HTTP認証を使用できますか？"

#: ../../faq.rst:165
msgid ""
"Yes, see "
":class:`~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware`."
msgstr "はい。 :class:`~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware` 参照。"

#: ../../faq.rst:168
msgid "Why does Scrapy download pages in English instead of my native language?"
msgstr "Scrapyが母国語ではなく英語でページをダウンロードするのはなぜですか？"

#: ../../faq.rst:170
msgid ""
"Try changing the default `Accept-Language`_ request header by overriding "
"the :setting:`DEFAULT_REQUEST_HEADERS` setting."
msgstr ""
":setting:`DEFAULT_REQUEST_HEADERS` 設定をオーバーライドして、デフォルトの `Accept-Language`_"
" リクエスト・ヘッダーを変更してみてください。"

#: ../../faq.rst:176
msgid "Where can I find some example Scrapy projects?"
msgstr "Scrapyプロジェクトの例はどこにありますか？"

#: ../../faq.rst:178
msgid "See :ref:`intro-examples`."
msgstr ":ref:`intro-examples` 参照。"

#: ../../faq.rst:181
msgid "Can I run a spider without creating a project?"
msgstr "プロジェクトを作成せずにスパイダーを実行できますか？"

#: ../../faq.rst:183
msgid ""
"Yes. You can use the :command:`runspider` command. For example, if you "
"have a spider written in a ``my_spider.py`` file you can run it with::"
msgstr ""
"はい。 :command:`runspider` コマンドを使用できます。たとえば、 ``my_spider.py`` "
"ファイルにスパイダーが記述されている場合は、次のコマンドで実行できます::"

#: ../../faq.rst:188
msgid "See :command:`runspider` command for more info."
msgstr "詳細については :command:`runspider` を参照してください。"

#: ../../faq.rst:191
msgid "I get \"Filtered offsite request\" messages. How can I fix them?"
msgstr "\"Filtered offsite request\"(フィルターされたオフサイト要求)メッセージが表示されます。 どうすれば修正できますか？"

#: ../../faq.rst:193
msgid ""
"Those messages (logged with ``DEBUG`` level) don't necessarily mean there"
" is a problem, so you may not need to fix them."
msgstr "これらのメッセージ(DEBUGレベルでログに記録される)は、必ずしも問題があることを意味するわけではないため、修正する必要はありません。"

#: ../../faq.rst:196
msgid ""
"Those messages are thrown by the Offsite Spider Middleware, which is a "
"spider middleware (enabled by default) whose purpose is to filter out "
"requests to domains outside the ones covered by the spider."
msgstr "これらのメッセージは、オフサイト・スパイダー・ミドルウェアによって送出されます。オフサイト・スパイダー・ミドルウェアは、スパイダーの対象外のドメインへのリクエストをフィルター処理することを目的とするスパイダー・ミドルウェア(デフォルトで有効)です。"

#: ../../faq.rst:200
msgid ""
"For more info see: "
":class:`~scrapy.spidermiddlewares.offsite.OffsiteMiddleware`."
msgstr "詳細は :class:`~scrapy.spidermiddlewares.offsite.OffsiteMiddleware` を参照して下さい。"

#: ../../faq.rst:204
msgid "What is the recommended way to deploy a Scrapy crawler in production?"
msgstr "Scrapyクローラーを運用環境に展開する推奨方法は何ですか？"

#: ../../faq.rst:206
msgid "See :ref:`topics-deploy`."
msgstr ":ref:`topics-deploy` 参照。"

#: ../../faq.rst:209
msgid "Can I use JSON for large exports?"
msgstr "大規模なエクスポートにJSONを使用できますか？"

#: ../../faq.rst:211
msgid ""
"It'll depend on how large your output is. See :ref:`this warning <json-"
"with-large-data>` in :class:`~scrapy.exporters.JsonItemExporter` "
"documentation."
msgstr ""
"出力の大きさに依存します。 :class:`~scrapy.exporters.JsonItemExporter` 文書の :ref:`警告"
"<json-with-large-data>` を参照してください。"

#: ../../faq.rst:216
msgid "Can I return (Twisted) deferreds from signal handlers?"
msgstr "シグナルハンドラーから(Twisted)遅延(deferred)を返すことができますか？"

#: ../../faq.rst:218
msgid ""
"Some signals support returning deferreds from their handlers, others "
"don't. See the :ref:`topics-signals-ref` to know which ones."
msgstr "ハンドラーからの遅延(deferred)を返すことをサポートするシグナルもあれば、サポートしないシグナルもあります。 :ref:`topics-signals-ref` を参照して、どれがどれか確認してください。"

#: ../../faq.rst:222
msgid "What does the response status code 999 means?"
msgstr "レスポンス・ステータス・コード999の意味は何ですか？"

#: ../../faq.rst:224
msgid ""
"999 is a custom response status code used by Yahoo sites to throttle "
"requests. Try slowing down the crawling speed by using a download delay "
"of ``2`` (or higher) in your spider::"
msgstr "999は、リクエストを抑制するためにYahooサイトで使用されるカスタム・レスポンス・ステータス・コードです。スパイダーで ``2`` (またはそれ以上)のダウンロード遅延を使用して、クロール速度を遅くしてみてください::"

#: ../../faq.rst:236
msgid ""
"Or by setting a global download delay in your project with the "
":setting:`DOWNLOAD_DELAY` setting."
msgstr "または、 :setting:`DOWNLOAD_DELAY` 設定でプロジェクトのグローバル・ダウンロード遅延を設定します。"

#: ../../faq.rst:240
msgid "Can I call ``pdb.set_trace()`` from my spiders to debug them?"
msgstr "スパイダーから ``pdb.set_trace()`` を呼び出してデバッグできますか？"

#: ../../faq.rst:242
msgid ""
"Yes, but you can also use the Scrapy shell which allows you to quickly "
"analyze (and even modify) the response being processed by your spider, "
"which is, quite often, more useful than plain old ``pdb.set_trace()``."
msgstr ""
"はい。ただし、スパイダーによって処理されているレスポンスをすばやく分析(および変更)できるScrapyシェルを使用することもできます。これは、通常の"
" ``pdb.set_trace()`` よりも非常に便利です。"

#: ../../faq.rst:246
msgid "For more info see :ref:`topics-shell-inspect-response`."
msgstr "詳細は :ref:`topics-shell-inspect-response` を参照して下さい。"

#: ../../faq.rst:249
msgid "Simplest way to dump all my scraped items into a JSON/CSV/XML file?"
msgstr "スクレイピングしたすべてのアイテムをJSON/CSV/XMLファイルにダンプする最も簡単な方法は？"

#: ../../faq.rst:251
msgid "To dump into a JSON file::"
msgstr "JSONファイルにダンプするには::"

#: ../../faq.rst:255
msgid "To dump into a CSV file::"
msgstr "CSVファイルにダンプするには::"

#: ../../faq.rst:259
msgid "To dump into a XML file::"
msgstr "XMLファイルにダンプするには::"

#: ../../faq.rst:263
msgid "For more information see :ref:`topics-feed-exports`"
msgstr "詳細は :ref:`topics-feed-exports` を参照して下さい。"

#: ../../faq.rst:266
msgid "What's this huge cryptic ``__VIEWSTATE`` parameter used in some forms?"
msgstr "いくつかのフォームで使用されているこの巨大な ``__VIEWSTATE`` パラメーターは何ですか？"

#: ../../faq.rst:268
msgid ""
"The ``__VIEWSTATE`` parameter is used in sites built with ASP.NET/VB.NET."
" For more info on how it works see http://search.cpan.org/~ecarroll/HTML-"
"TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm . Also, here's"
" an `example spider`_ which scrapes one of these sites."
msgstr "``__VIEWSTATE`` パラメーターは、ASP.NET/VB.NETで構築されたサイトで使用されます。動作の詳細については、http://search.cpan.org/~ecarroll/HTML-TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm を参照してください。また、これらのサイトの1つをスクレイピングする `example spider`_ もあります。"

#: ../../faq.rst:276
msgid "What's the best way to parse big XML/CSV data feeds?"
msgstr "大きなXML/CSVデータ・フィードを解析する最良の方法は何ですか？"

#: ../../faq.rst:278
msgid ""
"Parsing big feeds with XPath selectors can be problematic since they need"
" to build the DOM of the entire feed in memory, and this can be quite "
"slow and consume a lot of memory."
msgstr "XPathセレクターを使用して大きなフィードを解析すると、フィード全体のDOMをメモリに構築する必要があるため問題が発生する可能性があります。これは非常に遅く、大量のメモリを消費する可能性があります。"

#: ../../faq.rst:282
msgid ""
"In order to avoid parsing all the entire feed at once in memory, you can "
"use the functions ``xmliter`` and ``csviter`` from "
"``scrapy.utils.iterators`` module. In fact, this is what the feed spiders"
" (see :ref:`topics-spiders`) use under the cover."
msgstr "メモリ内のフィード全体を一度に解析することを避けるために、 ``scrapy.utils.iterators`` モジュールの関数 ``xmliter`` と ``csviter`` を使用できます。 実際、これはフィード・スパイダー( :ref:`topics-spiders` 参照)が内部で使用しているものです。"

#: ../../faq.rst:288
msgid "Does Scrapy manage cookies automatically?"
msgstr "Scrapyはクッキーを自動的に管理しますか？"

#: ../../faq.rst:290
msgid ""
"Yes, Scrapy receives and keeps track of cookies sent by servers, and "
"sends them back on subsequent requests, like any regular web browser "
"does."
msgstr "はい、Scrapyはサーバーから送信されたクッキーを受信して追跡し、通常のWebブラウザーが行うように、後続のリクエストでそれらを送り返します。"

#: ../../faq.rst:293
msgid "For more info see :ref:`topics-request-response` and :ref:`cookies-mw`."
msgstr "詳細は :ref:`topics-request-response` と :ref:`cookies-mw` を参照下さい。"

#: ../../faq.rst:296
msgid "How can I see the cookies being sent and received from Scrapy?"
msgstr "Scrapyとの間で送受信されているCookieを確認するにはどうすればよいですか？"

#: ../../faq.rst:298
msgid "Enable the :setting:`COOKIES_DEBUG` setting."
msgstr ":setting:`COOKIES_DEBUG` 設定を有効にします。"

#: ../../faq.rst:301
msgid "How can I instruct a spider to stop itself?"
msgstr "スパイダーに自分自身を止めるように指示するにはどうすればよいですか？"

#: ../../faq.rst:303
msgid ""
"Raise the :exc:`~scrapy.exceptions.CloseSpider` exception from a "
"callback. For more info see: :exc:`~scrapy.exceptions.CloseSpider`."
msgstr "コールバックから :exc:`~scrapy.exceptions.CloseSpider` 例外を発生させます。 詳細については、 :exc:`~scrapy.exceptions.CloseSpider` を参照してください。"

#: ../../faq.rst:307
msgid "How can I prevent my Scrapy bot from getting banned?"
msgstr "Scrapyボットがバン(BAN)されるのを防ぐにはどうすればよいですか？"

#: ../../faq.rst:309
msgid "See :ref:`bans`."
msgstr ":ref:`bans` 参照。"

#: ../../faq.rst:312
msgid "Should I use spider arguments or settings to configure my spider?"
msgstr "スパイダーを設定するには、スパイダーの引数または設定を使用する必要がありますか？"

#: ../../faq.rst:314
msgid ""
"Both :ref:`spider arguments <spiderargs>` and :ref:`settings <topics-"
"settings>` can be used to configure your spider. There is no strict rule "
"that mandates to use one or the other, but settings are more suited for "
"parameters that, once set, don't change much, while spider arguments are "
"meant to change more often, even on each spider run and sometimes are "
"required for the spider to run at all (for example, to set the start url "
"of a spider)."
msgstr ":ref:`スパイダー引数<spiderargs>` と :ref:`設定<topics-settings>` の両方を使用して、スパイダーを設定できます。どちらか一方を使用することを義務付ける厳密なルールはありませんが、設定は一度設定するとあまり変化しないパラメーターに適しています。一方、スパイダーの引数はスパイダーの実行ごとに頻繁に変更されることを意図しており、スパイダーを実行するには(たとえば、スパイダーの開始URLを設定するためなど、)どうせ必要になります。"

#: ../../faq.rst:321
msgid ""
"To illustrate with an example, assuming you have a spider that needs to "
"log into a site to scrape data, and you only want to scrape data from a "
"certain section of the site (which varies each time). In that case, the "
"credentials to log in would be settings, while the url of the section to "
"scrape would be a spider argument."
msgstr "例で説明するために、データをスクレイピングするためにサイトにログインする必要があるスパイダーがあり、(毎回異なる)サイトの特定のセクションからのみデータをスクレイピングしたいとします。 その場合、ログインする資格情報は設定になり、スクレイピングするセクションのURLはスパイダー引数になります。"

#: ../../faq.rst:328
msgid "I'm scraping a XML document and my XPath selector doesn't return any items"
msgstr "XMLドキュメントをスクレイピングしていますが、XPathセレクターはアイテムを返しません"

#: ../../faq.rst:330
msgid "You may need to remove namespaces. See :ref:`removing-namespaces`."
msgstr "名前空間を削除する必要がある場合があります。 :ref:`removing-namespaces` 参照。"

#: ../../faq.rst:335
msgid "How to split an item into multiple items in an item pipeline?"
msgstr "アイテム・パイプラインでアイテムを複数のアイテムに分割する方法は？"

#: ../../faq.rst:337
msgid ""
":ref:`Item pipelines <topics-item-pipeline>` cannot yield multiple items "
"per input item. :ref:`Create a spider middleware <custom-spider-"
"middleware>` instead, and use its "
":meth:`~scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output` "
"method for this puspose. For example::"
msgstr ":ref:`アイテム・パイプライン<topics-item-pipeline>` は、入力アイテムごとに複数のアイテムを生成できません。 代わりに :ref:`スパイダー・ミドルウェア<custom-spider-middleware>` を作成し、この目的で :meth:`~scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output` メソッドを使用します。例えば以下です::"

