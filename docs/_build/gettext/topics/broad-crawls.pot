# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008–2018, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Scrapy \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-09-10 09:37+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../topics/broad-crawls.rst:5
msgid "Broad Crawls"
msgstr ""

#: ../../topics/broad-crawls.rst:7
msgid "Scrapy defaults are optimized for crawling specific sites. These sites are often handled by a single Scrapy spider, although this is not necessary or required (for example, there are generic spiders that handle any given site thrown at them)."
msgstr ""

#: ../../topics/broad-crawls.rst:12
msgid "In addition to this \"focused crawl\", there is another common type of crawling which covers a large (potentially unlimited) number of domains, and is only limited by time or other arbitrary constraint, rather than stopping when the domain was crawled to completion or when there are no more requests to perform. These are called \"broad crawls\" and is the typical crawlers employed by search engines."
msgstr ""

#: ../../topics/broad-crawls.rst:19
msgid "These are some common properties often found in broad crawls:"
msgstr ""

#: ../../topics/broad-crawls.rst:21
msgid "they crawl many domains (often, unbounded) instead of a specific set of sites"
msgstr ""

#: ../../topics/broad-crawls.rst:23
msgid "they don't necessarily crawl domains to completion, because it would be impractical (or impossible) to do so, and instead limit the crawl by time or number of pages crawled"
msgstr ""

#: ../../topics/broad-crawls.rst:27
msgid "they are simpler in logic (as opposed to very complex spiders with many extraction rules) because data is often post-processed in a separate stage"
msgstr ""

#: ../../topics/broad-crawls.rst:30
msgid "they crawl many domains concurrently, which allows them to achieve faster crawl speeds by not being limited by any particular site constraint (each site is crawled slowly to respect politeness, but many sites are crawled in parallel)"
msgstr ""

#: ../../topics/broad-crawls.rst:35
msgid "As said above, Scrapy default settings are optimized for focused crawls, not broad crawls. However, due to its asynchronous architecture, Scrapy is very well suited for performing fast broad crawls. This page summarizes some things you need to keep in mind when using Scrapy for doing broad crawls, along with concrete suggestions of Scrapy settings to tune in order to achieve an efficient broad crawl."
msgstr ""

#: ../../topics/broad-crawls.rst:45
msgid "Use the right :setting:`SCHEDULER_PRIORITY_QUEUE`"
msgstr ""

#: ../../topics/broad-crawls.rst:47
msgid "Scrapy’s default scheduler priority queue is ``'scrapy.pqueues.ScrapyPriorityQueue'``. It works best during single-domain crawl. It does not work well with crawling many different domains in parallel"
msgstr ""

#: ../../topics/broad-crawls.rst:51
msgid "To apply the recommended priority queue use::"
msgstr ""

#: ../../topics/broad-crawls.rst:58
msgid "Increase concurrency"
msgstr ""

#: ../../topics/broad-crawls.rst:60
msgid "Concurrency is the number of requests that are processed in parallel. There is a global limit (:setting:`CONCURRENT_REQUESTS`) and an additional limit that can be set either per domain (:setting:`CONCURRENT_REQUESTS_PER_DOMAIN`) or per IP (:setting:`CONCURRENT_REQUESTS_PER_IP`)."
msgstr ""

#: ../../topics/broad-crawls.rst:65
msgid "The scheduler priority queue :ref:`recommended for broad crawls <broad-crawls-scheduler-priority-queue>` does not support :setting:`CONCURRENT_REQUESTS_PER_IP`."
msgstr ""

#: ../../topics/broad-crawls.rst:69
msgid "The default global concurrency limit in Scrapy is not suitable for crawling many different domains in parallel, so you will want to increase it. How much to increase it will depend on how much CPU and memory you crawler will have available."
msgstr ""

#: ../../topics/broad-crawls.rst:74
msgid "A good starting point is ``100``::"
msgstr ""

#: ../../topics/broad-crawls.rst:78
msgid "But the best way to find out is by doing some trials and identifying at what concurrency your Scrapy process gets CPU bounded. For optimum performance, you should pick a concurrency where CPU usage is at 80-90%."
msgstr ""

#: ../../topics/broad-crawls.rst:82
msgid "Increasing concurrency also increases memory usage. If memory usage is a concern, you might need to lower your global concurrency limit accordingly."
msgstr ""

#: ../../topics/broad-crawls.rst:87
msgid "Increase Twisted IO thread pool maximum size"
msgstr ""

#: ../../topics/broad-crawls.rst:89
msgid "Currently Scrapy does DNS resolution in a blocking way with usage of thread pool. With higher concurrency levels the crawling could be slow or even fail hitting DNS resolver timeouts. Possible solution to increase the number of threads handling DNS queries. The DNS queue will be processed faster speeding up establishing of connection and crawling overall."
msgstr ""

#: ../../topics/broad-crawls.rst:95
msgid "To increase maximum thread pool size use::"
msgstr ""

#: ../../topics/broad-crawls.rst:100
msgid "Setup your own DNS"
msgstr ""

#: ../../topics/broad-crawls.rst:102
msgid "If you have multiple crawling processes and single central DNS, it can act like DoS attack on the DNS server resulting to slow down of entire network or even blocking your machines. To avoid this setup your own DNS server with local cache and upstream to some large DNS like OpenDNS or Verizon."
msgstr ""

#: ../../topics/broad-crawls.rst:108
msgid "Reduce log level"
msgstr ""

#: ../../topics/broad-crawls.rst:110
msgid "When doing broad crawls you are often only interested in the crawl rates you get and any errors found. These stats are reported by Scrapy when using the ``INFO`` log level. In order to save CPU (and log storage requirements) you should not use ``DEBUG`` log level when preforming large broad crawls in production. Using ``DEBUG`` level when developing your (broad) crawler may be fine though."
msgstr ""

#: ../../topics/broad-crawls.rst:117
msgid "To set the log level use::"
msgstr ""

#: ../../topics/broad-crawls.rst:122
msgid "Disable cookies"
msgstr ""

#: ../../topics/broad-crawls.rst:124
msgid "Disable cookies unless you *really* need. Cookies are often not needed when doing broad crawls (search engine crawlers ignore them), and they improve performance by saving some CPU cycles and reducing the memory footprint of your Scrapy crawler."
msgstr ""

#: ../../topics/broad-crawls.rst:129
msgid "To disable cookies use::"
msgstr ""

#: ../../topics/broad-crawls.rst:134
msgid "Disable retries"
msgstr ""

#: ../../topics/broad-crawls.rst:136
msgid "Retrying failed HTTP requests can slow down the crawls substantially, specially when sites causes are very slow (or fail) to respond, thus causing a timeout error which gets retried many times, unnecessarily, preventing crawler capacity to be reused for other domains."
msgstr ""

#: ../../topics/broad-crawls.rst:141
msgid "To disable retries use::"
msgstr ""

#: ../../topics/broad-crawls.rst:146
msgid "Reduce download timeout"
msgstr ""

#: ../../topics/broad-crawls.rst:148
msgid "Unless you are crawling from a very slow connection (which shouldn't be the case for broad crawls) reduce the download timeout so that stuck requests are discarded quickly and free up capacity to process the next ones."
msgstr ""

#: ../../topics/broad-crawls.rst:152
msgid "To reduce the download timeout use::"
msgstr ""

#: ../../topics/broad-crawls.rst:157
msgid "Disable redirects"
msgstr ""

#: ../../topics/broad-crawls.rst:159
msgid "Consider disabling redirects, unless you are interested in following them. When doing broad crawls it's common to save redirects and resolve them when revisiting the site at a later crawl. This also help to keep the number of request constant per crawl batch, otherwise redirect loops may cause the crawler to dedicate too many resources on any specific domain."
msgstr ""

#: ../../topics/broad-crawls.rst:165
msgid "To disable redirects use::"
msgstr ""

#: ../../topics/broad-crawls.rst:170
msgid "Enable crawling of \"Ajax Crawlable Pages\""
msgstr ""

#: ../../topics/broad-crawls.rst:172
msgid "Some pages (up to 1%, based on empirical data from year 2013) declare themselves as `ajax crawlable`_. This means they provide plain HTML version of content that is usually available only via AJAX. Pages can indicate it in two ways:"
msgstr ""

#: ../../topics/broad-crawls.rst:177
msgid "by using ``#!`` in URL - this is the default way;"
msgstr ""

#: ../../topics/broad-crawls.rst:178
msgid "by using a special meta tag - this way is used on \"main\", \"index\" website pages."
msgstr ""

#: ../../topics/broad-crawls.rst:181
msgid "Scrapy handles (1) automatically; to handle (2) enable :ref:`AjaxCrawlMiddleware <ajaxcrawl-middleware>`::"
msgstr ""

#: ../../topics/broad-crawls.rst:186
msgid "When doing broad crawls it's common to crawl a lot of \"index\" web pages; AjaxCrawlMiddleware helps to crawl them correctly. It is turned OFF by default because it has some performance overhead, and enabling it for focused crawls doesn't make much sense."
msgstr ""

#: ../../topics/broad-crawls.rst:196
msgid "Crawl in BFO order"
msgstr ""

#: ../../topics/broad-crawls.rst:198
msgid ":ref:`Scrapy crawls in DFO order by default <faq-bfo-dfo>`."
msgstr ""

#: ../../topics/broad-crawls.rst:200
msgid "In broad crawls, however, page crawling tends to be faster than page processing. As a result, unprocessed early requests stay in memory until the final depth is reached, which can significantly increase memory usage."
msgstr ""

#: ../../topics/broad-crawls.rst:204
msgid ":ref:`Crawl in BFO order <faq-bfo-dfo>` instead to save memory."
msgstr ""

#: ../../topics/broad-crawls.rst:208
msgid "Be mindful of memory leaks"
msgstr ""

#: ../../topics/broad-crawls.rst:210
msgid "If your broad crawl shows a high memory usage, in addition to :ref:`crawling in BFO order <broad-crawls-bfo>` and :ref:`lowering concurrency <broad-crawls-concurrency>` you should :ref:`debug your memory leaks <topics-leaks>`."
msgstr ""

